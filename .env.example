# RAG Ops Platform â€” Local defaults

# Database (Docker local)
STORAGE_BACKEND=auto
LOCAL_DB_PATH=.ragops/ragops.db
DB_HOST=localhost
DB_PORT=5432
DB_NAME=ragops
DB_USER=ragops
DB_PASSWORD=ragops

# Optional external connection string (Neon/Aurora)
DATABASE_URL=
NEON_CONNECTION_STRING=

# Embeddings
EMBEDDING_PROVIDER=openai
OPENAI_API_KEY=
GEMINI_API_KEY=
HUGGINGFACE_API_KEY=
HUGGINGFACE_BASE_URL=https://api-inference.huggingface.co
HUGGINGFACE_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
HUGGINGFACE_EMBEDDING_DIMENSION=384
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large

# LLM
LLM_ENABLED=false
LLM_PROVIDER=openai
ANTHROPIC_API_KEY=
GROQ_API_KEY=
OLLAMA_LLM_MODEL=llama3

# Retrieval
TOP_K=5
CHUNK_SIZE=512
CHUNK_OVERLAP=64
CHAT_HISTORY_TURNS=6

# AWS
S3_BUCKET=
AWS_REGION=us-east-1

# App
LOG_LEVEL=INFO
ENVIRONMENT=local
REPO_ONBOARDING_ENABLED=false
REPO_CACHE_DIR=/tmp/ragops/repos
REPO_MANUALS_DIR=/tmp/ragops/manuals
REPO_ARCHIVE_MAX_MB=80
REPO_ONBOARDING_TIMEOUT_SECONDS=60
GITHUB_TOKEN=

# Access control (optional)
API_AUTH_ENABLED=false
API_KEYS_JSON={}
